require(xgboost)
require(methods)


begTime=Sys.time()

cat('Reading train data file ...');flush.console()
train = read.csv('Data/train.csv',header=TRUE,stringsAsFactors = F)
cat(paste('time used:',format(Sys.time()-begTime),'\n'));flush.console()
cat('Reading test data file ...');flush.console()
test = read.csv('Data/test.csv',header=TRUE,stringsAsFactors = F)
cat(paste('time used:',format(Sys.time()-begTime),'\n'));flush.console()
train = train[,-1]
test = test[,-1]

y = train[,ncol(train)]
y = gsub('Class_','',y)
y = as.integer(y)-1 #xgboost take features in [0,numOfClass)

x = rbind(train[,-ncol(train)],test)
x = as.matrix(x)
x = matrix(as.numeric(x),nrow(x),ncol(x))
trind = 1:length(y)
teind = (nrow(train)+1):nrow(x)

eta_range=c(0.01, 0.05, 0.1, 0.2, 0.3, 0.5, 1, 2, 5)
eta_range=c(0.2)
#   nround  eta depth lambda lambda_bias alpha       LL0     err0       LL1     err1      time
#12    150 0.20     6      0           0     0  0.247528 0.002466  0.499471 0.008676  1.688597
#15    150 0.30     6      0           0     0  0.183072 0.001593  0.501585 0.005817  1.735733
#14    100 0.30     6      0           0     0  0.246676 0.002940  0.501872 0.005664  1.735733
#11    100 0.20     6      0           0     0  0.305605 0.003167  0.510325 0.008727  1.688597
#9     150 0.10     6      0           0     0  0.350009 0.002124  0.520089 0.005496  1.767001
#16     50 0.50     6      0           0     0  0.264597 0.005643  0.522061 0.010323  1.954995
#13     50 0.30     6      0           0     0  0.343167 0.005562  0.524058 0.004696  1.735733
#17    100 0.50     6      0           0     0  0.160534 0.000312  0.525279 0.010514  1.954995
#8     100 0.10     6      0           0     0  0.408210 0.002929  0.545236 0.005067  1.767001
#10     50 0.20     6      0           0     0  0.404427 0.002133  0.546332 0.008920  1.688597
#18    150 0.50     6      0           0     0  0.105399 0.001305  0.547844 0.011040  1.954995
#6     150 0.05     6      0           0     0  0.456610 0.001377  0.572396 0.005432  1.767701
#7      50 0.10     6      0           0     0  0.528497 0.003309  0.622215 0.004804  1.767001
#5     100 0.05     6      0           0     0  0.531805 0.001894  0.624477 0.005394  1.767701
#19     50 1.00     6      0           0     0  0.149696 0.002390  0.628393 0.008420  1.709564
#20    100 1.00     6      0           0     0  0.065916 0.000607  0.692013 0.008820  1.709564
#21    150 1.00     6      0           0     0  0.032067 0.000917  0.754535 0.008567  1.709564
#4      50 0.05     6      0           0     0  0.734040 0.003047  0.793005 0.004722  1.767701
#3     150 0.01     6      0           0     0  0.957228 0.002968  0.999693 0.005164  1.757551
#2     100 0.01     6      0           0     0  1.153398 0.003339  1.185622 0.003549  1.757551
#1      50 0.01     6      0           0     0  1.481284 0.002065  1.500892 0.002517  1.757551

depth_range=c(1,2,3,4,5,6,7,8,9,10)
depth_range=c(8)
#   nround eta depth lambda lambda_bias alpha      LL0     err0      LL1     err1      time
#23    100 0.2     8      0           0     0 0.185207 0.001449 0.498273 0.000886  2.325483
#21    150 0.2     7      0           0     0 0.182463 0.000364 0.499334 0.007398  2.022599
#24    150 0.2     8      0           0     0 0.129011 0.002100 0.502354 0.000260  2.325483
#18    150 0.2     6      0           0     0 0.249146 0.002629 0.502889 0.003503  1.725949
#20    100 0.2     7      0           0     0 0.241005 0.002288 0.503297 0.006817  2.022599
#26    100 0.2     9      0           0     0 0.141071 0.002716 0.504964 0.007716  2.613749
#15    150 0.2     5      0           0     0 0.324246 0.002219 0.508667 0.005783  1.462167
#29    100 0.2    10      0           0     0 0.104656 0.003083 0.512179 0.011069  2.808611
#17    100 0.2     6      0           0     0 0.306720 0.003950 0.512192 0.002808  1.725949
#28     50 0.2    10      0           0     0 0.193954 0.002960 0.516415 0.009761  2.808611
#27    150 0.2     9      0           0     0 0.089227 0.001922 0.517157 0.008204  2.613749
#25     50 0.2     9      0           0     0 0.236108 0.004623 0.517875 0.007286  2.613749
#22     50 0.2     8      0           0     0 0.285130 0.002484 0.520253 0.002232  2.325483
#14    100 0.2     5      0           0     0 0.377206 0.001841 0.524998 0.005018  1.462167
#12    150 0.2     4      0           0     0 0.403325 0.001227 0.529025 0.002139  1.216620
#30    150 0.2    10      0           0     0 0.061920 0.002987 0.529360 0.012559  2.808611
#19     50 0.2     7      0           0     0 0.342388 0.003356 0.532469 0.005020  2.022599
#16     50 0.2     6      0           0     0 0.405226 0.003017 0.549053 0.002265  1.725949
#11    100 0.2     4      0           0     0 0.452007 0.001087 0.549741 0.002364  1.216620
#9     150 0.2     3      0           0     0 0.486182 0.003503 0.556768 0.005517  1.063811
#13     50 0.2     5      0           0     0 0.471834 0.002268 0.569530 0.004219  1.462167
#8     100 0.2     3      0           0     0 0.531033 0.003625 0.583735 0.005578  1.063811
#10     50 0.2     4      0           0     0 0.540366 0.001647 0.602350 0.003456  1.216620
#6     150 0.2     2      0           0     0 0.577631 0.002534 0.611224 0.004161 44.294533
#5     100 0.2     2      0           0     0 0.623078 0.001884 0.647843 0.004711 44.294533
#7      50 0.2     3      0           0     0 0.618887 0.003063 0.650380 0.006082  1.063811
#3     150 0.2     1      0           0     0 0.720408 0.000348 0.731001 0.001582 30.290732
#4      50 0.2     2      0           0     0 0.718835 0.001920 0.732921 0.004521 44.294533
#2     100 0.2     1      0           0     0 0.782715 0.000098 0.790297 0.001919 30.290732
#1      50 0.2     1      0           0     0 0.917947 0.000704 0.922648 0.003079 30.290732

lambda_range=seq(0,1,0.1)
lambda_range=c(1)
#   nround eta depth lambda lambda_bias alpha      LL0     err0      LL1     err1     time
#33    150 0.2     8    1.0           0     0 0.148439 0.001322 0.492553 0.005946 2.088186
#27    150 0.2     8    0.8           0     0 0.146148 0.001366 0.493172 0.006074 2.002781
#26    100 0.2     8    0.8           0     0 0.203515 0.001212 0.494135 0.004935 2.002781
#30    150 0.2     8    0.9           0     0 0.149685 0.004049 0.494188 0.005074 2.024216
#32    100 0.2     8    1.0           0     0 0.207057 0.002090 0.494199 0.005150 2.088186
#23    100 0.2     8    0.7           0     0 0.202930 0.004214 0.494794 0.004518 1.964446
#24    150 0.2     8    0.7           0     0 0.144886 0.003005 0.494818 0.006111 1.964446
#29    100 0.2     8    0.9           0     0 0.205224 0.003553 0.494833 0.003666 2.024216
#17    100 0.2     8    0.5           0     0 0.194874 0.003170 0.495747 0.002821 1.959662
#11    100 0.2     8    0.3           0     0 0.193201 0.003383 0.495916 0.004582 1.959312
#18    150 0.2     8    0.5           0     0 0.139176 0.002797 0.496153 0.003941 1.959662
#14    100 0.2     8    0.4           0     0 0.195916 0.004426 0.496233 0.005134 1.962729
#20    100 0.2     8    0.6           0     0 0.200637 0.003573 0.496343 0.004332 1.955212
#21    150 0.2     8    0.6           0     0 0.142116 0.004981 0.496344 0.006608 1.955212
#5     100 0.2     8    0.1           0     0 0.185821 0.000625 0.496758 0.005150 1.999848
#8     100 0.2     8    0.2           0     0 0.190416 0.004031 0.496871 0.005489 2.036416
#15    150 0.2     8    0.4           0     0 0.138949 0.003856 0.496925 0.007359 1.962729
#12    150 0.2     8    0.3           0     0 0.136272 0.003214 0.497189 0.007266 1.959312
#9     150 0.2     8    0.2           0     0 0.133877 0.003971 0.498682 0.006779 2.036416
#6     150 0.2     8    0.1           0     0 0.128778 0.000861 0.499737 0.006074 1.999848
#2     100 0.2     8    0.0           0     0 0.182776 0.005268 0.500583 0.005691 2.058968
#3     150 0.2     8    0.0           0     0 0.125961 0.001969 0.504332 0.007653 2.058968
#13     50 0.2     8    0.4           0     0 0.295792 0.002284 0.519536 0.004060 1.962729
#25     50 0.2     8    0.8           0     0 0.303299 0.004348 0.519831 0.002876 2.002781
#16     50 0.2     8    0.5           0     0 0.295479 0.003903 0.520027 0.003340 1.959662
#31     50 0.2     8    1.0           0     0 0.306681 0.002853 0.520118 0.003991 2.088186
#7      50 0.2     8    0.2           0     0 0.292876 0.005922 0.520218 0.003165 2.036416
#19     50 0.2     8    0.6           0     0 0.299322 0.004898 0.520319 0.002865 1.955212
#28     50 0.2     8    0.9           0     0 0.306229 0.004736 0.520404 0.002985 2.024216
#10     50 0.2     8    0.3           0     0 0.295426 0.002519 0.520511 0.004609 1.959312
#4      50 0.2     8    0.1           0     0 0.290509 0.000767 0.520747 0.005146 1.999848
#22     50 0.2     8    0.7           0     0 0.303000 0.005828 0.520825 0.003414 1.964446
#1      50 0.2     8    0.0           0     0 0.286733 0.003044 0.521350 0.003893 2.058968

alpha_range=c(0)
alpha_range=seq(0,1,0.2)
alpha_range=c(0.8)
#   nround eta depth lambda lambda_bias alpha      LL0     err0      LL1     err1     time
#15    150 0.2     8      1           0   0.8 0.142027 0.002217 0.486249 0.005972 2.048484
#18    150 0.2     8      1           0   1.0 0.142287 0.002399 0.487294 0.006495 2.047500
#12    150 0.2     8      1           0   0.6 0.139571 0.003298 0.488565 0.005501 2.065385
#14    100 0.2     8      1           0   0.8 0.206111 0.002980 0.489283 0.005165 2.048484
#9     150 0.2     8      1           0   0.4 0.141937 0.002886 0.490184 0.005302 2.056918
#11    100 0.2     8      1           0   0.6 0.203878 0.002696 0.490515 0.003391 2.065385
#17    100 0.2     8      1           0   1.0 0.207321 0.002493 0.490861 0.004772 2.047500
#8     100 0.2     8      1           0   0.4 0.203397 0.004949 0.492297 0.003265 2.056918
#6     150 0.2     8      1           0   0.2 0.142353 0.002491 0.492427 0.005641 1.981597
#3     150 0.2     8      1           0   0.0 0.148439 0.001322 0.492553 0.005946 1.953195
#5     100 0.2     8      1           0   0.2 0.200831 0.004133 0.493925 0.003523 1.981597
#2     100 0.2     8      1           0   0.0 0.207057 0.002090 0.494199 0.005150 1.953195
#13     50 0.2     8      1           0   0.8 0.316754 0.004139 0.519442 0.003183 2.048484
#10     50 0.2     8      1           0   0.6 0.313237 0.005455 0.519777 0.001990 2.065385
#1      50 0.2     8      1           0   0.0 0.306681 0.002853 0.520118 0.003991 1.953195
#7      50 0.2     8      1           0   0.4 0.310253 0.004792 0.520338 0.002272 2.056918
#4      50 0.2     8      1           0   0.2 0.306986 0.004954 0.520767 0.002109 1.981597
#16     50 0.2     8      1           0   1.0 0.321295 0.003677 0.521442 0.002946 2.047500

lambda_bias_range=seq(0,1,0.2)
lambda_bias_range=c(0)

child_range=c(0.5, 1, 2, 3, 4, 5, 10)
child_range=c(3)
#   nround eta depth lambda lambda_bias alpha childwgt      LL0     err0      LL1     err1     time
#12    150 0.2     8      1           0   0.8      3.0 0.170600 0.002807 0.484745 0.004989 1.944228
#18    150 0.2     8      1           0   0.8      5.0 0.192785 0.005397 0.484793 0.005079 1.902792
#15    150 0.2     8      1           0   0.8      4.0 0.181043 0.002185 0.484851 0.005095 1.922777
#9     150 0.2     8      1           0   0.8      2.0 0.155439 0.001744 0.485483 0.005773 1.987264
#6     150 0.2     8      1           0   0.8      1.0 0.142027 0.002217 0.486249 0.005972 2.057351
#21    150 0.2     8      1           0   0.8     10.0 0.231000 0.003295 0.486291 0.005717 1.840355
#3     150 0.2     8      1           0   0.8      0.5 0.134033 0.001811 0.489121 0.007425 2.101220
#5     100 0.2     8      1           0   0.8      1.0 0.206111 0.002980 0.489283 0.005165 2.057351
#8     100 0.2     8      1           0   0.8      2.0 0.218270 0.002470 0.489966 0.004600 1.987264
#11    100 0.2     8      1           0   0.8      3.0 0.232140 0.003715 0.490574 0.004325 1.944228
#2     100 0.2     8      1           0   0.8      0.5 0.198557 0.003257 0.491048 0.005281 2.101220
#14    100 0.2     8      1           0   0.8      4.0 0.242678 0.002644 0.491593 0.004019 1.922777
#17    100 0.2     8      1           0   0.8      5.0 0.253624 0.006055 0.492571 0.003663 1.902792
#20    100 0.2     8      1           0   0.8     10.0 0.287717 0.003904 0.496073 0.004663 1.840355
#4      50 0.2     8      1           0   0.8      1.0 0.316754 0.004139 0.519442 0.003183 2.057351
#1      50 0.2     8      1           0   0.8      0.5 0.312820 0.003610 0.520262 0.002912 2.101220
#7      50 0.2     8      1           0   0.8      2.0 0.328152 0.005305 0.520905 0.002882 1.987264
#10     50 0.2     8      1           0   0.8      3.0 0.339774 0.004255 0.522406 0.003172 1.944228
#13     50 0.2     8      1           0   0.8      4.0 0.347859 0.004866 0.523747 0.002302 1.922777
#16     50 0.2     8      1           0   0.8      5.0 0.356127 0.005034 0.525074 0.002163 1.902792
#19     50 0.2     8      1           0   0.8     10.0 0.387167 0.004087 0.530817 0.003171 1.840355
subsamp_range=seq(0.2,1,0.1)
subsamp_range=c(0.9)
#   nround eta depth lambda lambda_bias alpha childwgt subsamp      LL0     err0      LL1     err1     time
#24    150 0.2     8      1           0   0.8        3     0.9 0.154896 0.000936 0.482602 0.007148 2.014915
#27    150 0.2     8      1           0   0.8        3     1.0 0.170600 0.002807 0.484745 0.004989 2.002615
#18    150 0.2     8      1           0   0.8        3     0.7 0.149296 0.001586 0.484777 0.006854 2.000648
#21    150 0.2     8      1           0   0.8        3     0.8 0.149553 0.002167 0.485228 0.007920 1.991147
#17    100 0.2     8      1           0   0.8        3     0.7 0.216075 0.001750 0.488180 0.005327 2.000648
#23    100 0.2     8      1           0   0.8        3     0.9 0.219806 0.000609 0.488183 0.005947 2.014915
#20    100 0.2     8      1           0   0.8        3     0.8 0.216232 0.002731 0.488818 0.006131 1.991147
#15    150 0.2     8      1           0   0.8        3     0.6 0.153346 0.001732 0.489019 0.006399 1.969296
#26    100 0.2     8      1           0   0.8        3     1.0 0.232140 0.003715 0.490574 0.004325 2.002615
#14    100 0.2     8      1           0   0.8        3     0.6 0.219369 0.000810 0.491048 0.005907 1.969296
#12    150 0.2     8      1           0   0.8        3     0.5 0.161593 0.001937 0.492229 0.005638 1.855606
#11    100 0.2     8      1           0   0.8        3     0.5 0.227744 0.002093 0.495328 0.005710 1.855606
#9     150 0.2     8      1           0   0.8        3     0.4 0.174249 0.001017 0.499219 0.007119 1.624743
#8     100 0.2     8      1           0   0.8        3     0.4 0.240274 0.001149 0.501285 0.005365 1.624743
#6     150 0.2     8      1           0   0.8        3     0.3 0.198940 0.001798 0.507123 0.003852 1.424215
#5     100 0.2     8      1           0   0.8        3     0.3 0.264175 0.000965 0.510353 0.003347 1.424215
#16     50 0.2     8      1           0   0.8        3     0.7 0.334974 0.002232 0.518840 0.004627 2.000648
#19     50 0.2     8      1           0   0.8        3     0.8 0.333769 0.002978 0.519534 0.003908 1.991147
#22     50 0.2     8      1           0   0.8        3     0.9 0.333422 0.002875 0.520386 0.003732 2.014915
#13     50 0.2     8      1           0   0.8        3     0.6 0.340490 0.000941 0.520815 0.004618 1.969296
#25     50 0.2     8      1           0   0.8        3     1.0 0.339774 0.004255 0.522406 0.003172 2.002615
#10     50 0.2     8      1           0   0.8        3     0.5 0.348862 0.002067 0.524331 0.002873 1.855606
#3     150 0.2     8      1           0   0.8        3     0.2 0.240257 0.000777 0.526513 0.003231 1.168684
#2     100 0.2     8      1           0   0.8        3     0.2 0.304230 0.001248 0.528420 0.002675 1.168684
#7      50 0.2     8      1           0   0.8        3     0.4 0.361511 0.000715 0.529210 0.005431 1.624743
#4      50 0.2     8      1           0   0.8        3     0.3 0.382226 0.001194 0.537191 0.003043 1.424215
#1      50 0.2     8      1           0   0.8        3     0.2 0.414737 0.001612 0.552300 0.002314 1.168684

colsamp_range=seq(0.2,1,0.1)
colsamp_range=c(0.6)
#   nround eta depth lambda lambda_bias alpha childwgt subsamp colsamp      LL0     err0      LL1     err1     time
#15    150 0.2     8      1           0   0.8        3     0.9     0.6 0.179641 0.001131 0.479485 0.007360 1.440049
#12    150 0.2     8      1           0   0.8        3     0.9     0.5 0.188874 0.000280 0.479622 0.006256 1.332910
#21    150 0.2     8      1           0   0.8        3     0.9     0.8 0.165642 0.000302 0.480708 0.004750 1.743800
#9     150 0.2     8      1           0   0.8        3     0.9     0.4 0.204533 0.000294 0.481102 0.006265 1.182634
#18    150 0.2     8      1           0   0.8        3     0.9     0.7 0.170350 0.000734 0.481328 0.006167 1.605442
#24    150 0.2     8      1           0   0.8        3     0.9     0.9 0.159042 0.001835 0.482275 0.006265 1.848106
#27    150 0.2     8      1           0   0.8        3     0.9     1.0 0.154896 0.000936 0.482603 0.007148 2.008632
#6     150 0.2     8      1           0   0.8        3     0.9     0.3 0.225432 0.001973 0.486100 0.004304 1.058494
#20    100 0.2     8      1           0   0.8        3     0.9     0.8 0.231272 0.001540 0.487305 0.004154 1.743800
#17    100 0.2     8      1           0   0.8        3     0.9     0.7 0.235516 0.001179 0.488065 0.006278 1.605442
#23    100 0.2     8      1           0   0.8        3     0.9     0.9 0.224670 0.003428 0.488074 0.004732 1.848106
#26    100 0.2     8      1           0   0.8        3     0.9     1.0 0.219806 0.000609 0.488183 0.005947 2.008632
#14    100 0.2     8      1           0   0.8        3     0.9     0.6 0.244401 0.000702 0.488332 0.006114 1.440049
#11    100 0.2     8      1           0   0.8        3     0.9     0.5 0.252716 0.001911 0.489258 0.004794 1.332910
#8     100 0.2     8      1           0   0.8        3     0.9     0.4 0.267983 0.001187 0.492676 0.004655 1.182634
#3     150 0.2     8      1           0   0.8        3     0.9     0.2 0.262649 0.003392 0.495493 0.005234 1.239104
#5     100 0.2     8      1           0   0.8        3     0.9     0.3 0.288992 0.003119 0.500732 0.003505 1.058494
#2     100 0.2     8      1           0   0.8        3     0.9     0.2 0.326537 0.005116 0.516962 0.004589 1.239104
#19     50 0.2     8      1           0   0.8        3     0.9     0.8 0.341939 0.003229 0.519287 0.003003 1.743800
#22     50 0.2     8      1           0   0.8        3     0.9     0.9 0.337234 0.003543 0.520198 0.003519 1.848106
#25     50 0.2     8      1           0   0.8        3     0.9     1.0 0.333422 0.002875 0.520386 0.003732 2.008632
#16     50 0.2     8      1           0   0.8        3     0.9     0.7 0.345466 0.004250 0.521971 0.002632 1.605442
#13     50 0.2     8      1           0   0.8        3     0.9     0.6 0.354263 0.002319 0.524012 0.003571 1.440049
#10     50 0.2     8      1           0   0.8        3     0.9     0.5 0.360224 0.002141 0.526842 0.003552 1.332910
#7      50 0.2     8      1           0   0.8        3     0.9     0.4 0.375428 0.001257 0.535658 0.003422 1.182634
#4      50 0.2     8      1           0   0.8        3     0.9     0.3 0.402356 0.002305 0.552879 0.000867 1.058494
#1      50 0.2     8      1           0   0.8        3     0.9     0.2 0.459038 0.005703 0.591775 0.001345 1.239104

ratiof_range=c(0)

round_count=3

nv=rep(NA, round_count*length(eta_range)*length(depth_range)*length(lambda_range)*
            length(lambda_bias_range)*length(alpha_range)*length(child_range)*
            length(subsamp_range)*length(colsamp_range))

results=data.frame(nround=nv,eta=nv,depth=nv,lambda=nv,lambda_bias=nv,alpha=nv,
                        childwgt=nv, subsamp=nv, colsamp=nv,
                        LL0=nv,err0=nv,LL1=nv,err1=nv, time=nv)
# Run Cross Valication
cv.nround = 500
cv.nround = 150
seednum=20150320

ri=1
for (addratio in ratiof_range) {
    if (addratio) {
        train=add_ratio_features(train_ori)
    } else {
        train=train_ori
    }
for (myeta in eta_range) {
for (mydepth in depth_range) {
for (mylambda in lambda_range) {
for (mylambda_bias in lambda_bias_range) {
for (myalpha in alpha_range) {
for (mychild in child_range) {
for (mysubsamp in subsamp_range) {
for (mycolsamp in colsamp_range) {
    set.seed(seednum)
    begTime0=Sys.time()
# Set necessary parameter
    param <- list("objective" = "multi:softprob",
              "eval_metric" = "mlogloss",
              "num_class" = 9,
                "bst:eta" = myeta,
                "bst:max_depth" = mydepth,
                "lambda" = mylambda,
                "lambda_bias" = mylambda_bias,
                "alpha" = myalpha,
                "min_child_weight" = mychild,
                "subsample" = mysubsamp,
                "colsample_bytree" = mycolsamp,
              "nthread" = 8)

    cat('Constructing cross validation model ...');flush.console()
    bst.cv = xgb.cv(param=param, data = x[trind,], label = y, 
                 nfold = 3, nrounds=cv.nround)
    bst.cv=data.frame(bst.cv)
    mytime=Sys.time()-begTime0
    rownum=nrow(bst.cv)
    myrow=round(rownum/3)
    results[ri,]=c(myrow, myeta, mydepth, mylambda, mylambda_bias, myalpha, mychild, mysubsamp, mycolsamp,
                        bst.cv[myrow,], mytime)
    myrow=round(2*rownum/3)
    results[ri+1,]=c(myrow, myeta, mydepth, mylambda, mylambda_bias, myalpha, mychild, mysubsamp, mycolsamp,
                        bst.cv[myrow,], mytime)
    myrow=round(rownum)
    results[ri+2,]=c(myrow, myeta, mydepth, mylambda, mylambda_bias, myalpha, mychild, mysubsamp, mycolsamp,
                        bst.cv[myrow,], mytime)
    ri = ri + 3
    print(results[!is.na(results[,1]),]);flush.console()
    cat(paste('Validation with nround=',cv.nround,'eta=',myeta,'lambda=',mylambda,'lambda_bias=',mylambda_bias,
        'alpha=',myalpha,'childmaxweight=',mychild,'subsample=',mysubsamp,'colsamp=',mycolsamp,
        'took:',format(mytime),'\n'));flush.console()
}   # mycolsamp
}   # mysubsamp
}   # mychild
}   # myalpha
}   # mylambda_bias
}   # mylambda
}   # mydepth
}   # myeta
}   # addratio
results=results[!is.na(results[,1]),]
print(results[order(results$LL1),]);flush.console()
cat(paste('All cross-validation time used:',format(Sys.time()-begTime),'\n'));flush.console()

stop("After cross validation.")

cat('Train model ...');flush.console()
# Train the model
nround = 50
bst = xgboost(param=param, data = x[trind,], label = y, nrounds=nround)
cat(paste('time used:',format(Sys.time()-begTime),'\n'));flush.console()

cat('Predicting ...');flush.console()
# Make prediction
pred = predict(bst,x[teind,])
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
cat(paste('time used:',format(Sys.time()-begTime),'\n'));flush.console()

cat('Outputing ...');flush.console()
# Output submission
pred = format(pred, digits=2,scientific=F) # shrink the size of submission
pred = data.frame(1:nrow(pred),pred)
names(pred) = c('id', paste0('Class_',1:9))
write.csv(pred,file='submission.csv', quote=FALSE,row.names=FALSE)
cat(paste('time used:',format(Sys.time()-begTime),'\n'));flush.console()
